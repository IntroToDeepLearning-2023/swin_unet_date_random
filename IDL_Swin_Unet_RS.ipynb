{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **IDL Project: (random image selection in time dimension)**\n",
        "####**Crop Type Mapping: Satellite imagery-based crop type segmentation for small fields using deep learning neural networks**"
      ],
      "metadata": {
        "id": "GgD211Lu1Fy3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install visdom==0.1.7 wandb rasterio\n",
        "!pip install einops\n",
        "!pip install einsum"
      ],
      "metadata": {
        "id": "TUYOcElVgJTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NSIfpgc-esut"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional\n",
        "import torch.nn.functional as F\n",
        "from torch import autograd\n",
        "import torch.optim as optim\n",
        "\n",
        "import os\n",
        "from glob import glob\n",
        "from datetime import datetime\n",
        "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "from matplotlib import pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "\n",
        "import visdom\n",
        "from torchvision.utils import save_image, make_grid\n",
        "import datetime\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "import itertools\n",
        "import wandb\n",
        "import math\n",
        "from einops import rearrange\n",
        "\n",
        "import argparse\n",
        "import pandas as pd\n",
        "import time\n",
        "import random\n",
        "from datetime import datetime\n",
        "import rasterio\n",
        "from pathlib import Path\n",
        "\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.utils.rnn as rnn\n",
        "import json\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import WeightedRandomSampler, SubsetRandomSampler\n",
        "from torchvision import models\n",
        "from torchviz import make_dot, make_dot_from_trace\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", DEVICE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3dU0Zi89g1j",
        "outputId": "a48c90f8-82d2-4670-e1ac-2def15c84768"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device:  cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !gsutil ls gs://data_ctm/data/data/africa_crop_type_mapping/ghana/"
      ],
      "metadata": {
        "id": "Wbh4v4pEl5T2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BAND STATS\n",
        "\n",
        "BANDS = { 's1': { 'VV': 0, 'VH': 1, 'RATIO': 2},\n",
        "          's2': { '10': {'BLUE': 0, 'GREEN': 1, 'RED': 2, 'RDED1': 3, 'RDED2': 4, 'RDED3': 5, 'NIR': 6, 'RDED4': 7, 'SWIR1': 8, 'SWIR2': 9},\n",
        "                   '4': {'BLUE': 0, 'GREEN': 1, 'RED': 2, 'NIR': 3}},\n",
        "          'planet': { '4': {'BLUE': 0, 'GREEN': 1, 'RED': 2, 'NIR': 3}}}\n",
        "\n",
        "MEANS = { 's1': { 'ghana': torch.Tensor([-10.50, -17.24, 1.17])},\n",
        "          's2': { 'ghana': torch.Tensor([2620.00, 2519.89, 2630.31, 2739.81, 3225.22, 3562.64, 3356.57, 3788.05, 2915.40, 2102.65])},\n",
        "          'planet': { 'ghana': torch.Tensor([1264.81, 1255.25, 1271.10, 2033.22])},\n",
        "          's2_cldfltr': { 'ghana': torch.Tensor([1362.68, 1317.62, 1410.74, 1580.05, 2066.06, 2373.60, 2254.70, 2629.11, 2597.50, 1818.43])} }\n",
        "\n",
        "STDS = { 's1': { 'ghana': torch.Tensor([3.57, 4.86, 5.60])},\n",
        "         's2': { 'ghana': torch.Tensor([2171.62, 2085.69, 2174.37, 2084.56, 2058.97, 2117.31, 1988.70, 2099.78, 1209.48, 918.19])},\n",
        "         'planet': { 'ghana': torch.Tensor([602.51, 598.66, 637.06, 966.27])},\n",
        "         's2_cldfltr': { 'ghana': torch.Tensor([511.19, 495.87, 591.44, 590.27, 745.81, 882.05, 811.14, 959.09, 964.64, 809.53])} }\n",
        "\n",
        "\n",
        "CROPS = { 'ghana': ['groundnut', 'maize', 'rice', 'soya bean']}"
      ],
      "metadata": {
        "id": "1YOZuJC1SXNw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "kehCYvcxAl-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CropTypeMappingDataset():\n",
        "\n",
        "    def __init__(self, ):\n",
        "\n",
        "        self.data_dir = '/content/data/africa_crop_type_mapping' # 'gs://data_ctm/data/africa_crop_type_mapping'\n",
        "\n",
        "        self.split_dict = {'train': 0, 'val': 1, 'test': 2}\n",
        "        self.split_names = {'train': 'Train', 'val': 'Validation', 'test': 'Test'}\n",
        "\n",
        "        # Extract splits\n",
        "        split_df = pd.read_csv(os.path.join(self.data_dir, 'ghana', 'list_eval_partition.csv'))\n",
        "\n",
        "        self.split_array = split_df['partition'].values\n",
        "\n",
        "\n",
        "        # y_array stores idx ids corresponding to location. Actual y labels are\n",
        "        # tensors that are loaded separately.\n",
        "        self.y_array = torch.from_numpy(split_df['id'].values)\n",
        "\n",
        "        self.y_size = (64, 64)\n",
        "\n",
        "        self.metadata_fields = ['y']\n",
        "        self.metadata_array = torch.from_numpy(split_df['id'].values)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.get_input(idx)\n",
        "        y = self.get_label(idx)\n",
        "        return x, y\n",
        "\n",
        "\n",
        "    def get_input(self, idx):\n",
        "        \"\"\"\n",
        "        Returns X for a given idx.\n",
        "        \"\"\"\n",
        "        loc_id = f'{self.y_array[idx]:06d}'\n",
        "\n",
        "        images = np.load(os.path.join(self.data_dir, 'ghana', 'npy', f'{\"ghana\"}_{loc_id}.npz'))\n",
        "        dates_idx = self.get_dates(loc_id)\n",
        "\n",
        "        s1 = images['s1']\n",
        "        s2 = images['s2']\n",
        "        planet = images['planet']\n",
        "\n",
        "        s1 = torch.from_numpy(s1)\n",
        "        s2 = torch.from_numpy(s2.astype(np.int32))\n",
        "        planet = torch.from_numpy(planet.astype(np.int32))\n",
        "\n",
        "        planet = planet.permute(3, 0, 1, 2)\n",
        "        planet = transforms.CenterCrop(128)(planet)\n",
        "        planet = planet.permute(1, 2, 3, 0)\n",
        "\n",
        "        # Normalization\n",
        "        s1 = self.normalization(s1, 's1')\n",
        "        s2 = self.normalization(s2, 's2')\n",
        "        planet = self.normalization(planet, 'planet')\n",
        "\n",
        "        s1 = s1[:,:,:,dates_idx[\"s1_min\"]:dates_idx[\"s1_max\"]]\n",
        "        s2 = s2[:,:,:,dates_idx[\"s2_min\"]:dates_idx[\"s2_max\"]]\n",
        "        planet = planet[:,:,:,dates_idx[\"planet_min\"]:dates_idx[\"planet_max\"]]\n",
        "\n",
        "        # Randomly select image in time dimension\n",
        "        s1_d = random.randint(0, dates_idx[\"s1_max\"] - dates_idx[\"s1_min\"]-1)\n",
        "        s2_d = random.randint(0, dates_idx[\"s2_max\"] - dates_idx[\"s2_min\"]-1)\n",
        "        planet_d = random.randint(0, dates_idx[\"planet_max\"] - dates_idx[\"planet_min\"]-1)\n",
        "\n",
        "        s1 = np.squeeze(s1[:,:,:,s1_d])\n",
        "        s2 = np.squeeze(s2[:,:,:,s2_d])\n",
        "        planet = np.squeeze(planet[:,:,:,planet_d])\n",
        "\n",
        "\n",
        "        return {'s1': torch.tensor(s1, dtype=torch.float32), 's2': torch.tensor(s2, dtype=torch.float32), 'planet': torch.tensor(planet, dtype=torch.float32)}\n",
        "\n",
        "    def get_label(self, idx):\n",
        "        \"\"\"\n",
        "        Returns y for a given idx.\n",
        "        \"\"\"\n",
        "        loc_id = f'{self.y_array[idx]:06d}'\n",
        "        label = np.load(os.path.join(self.data_dir, 'ghana', 'truth', f'{\"ghana\"}_{loc_id}.npz'))['truth']\n",
        "        label = torch.from_numpy(label)\n",
        "        label[label>4]=0\n",
        "        return label\n",
        "\n",
        "    def get_dates(self, loc_id):\n",
        "        \"\"\"\n",
        "        Converts json dates into tensor containing dates\n",
        "        \"\"\"\n",
        "        s1_json = json.loads(open(os.path.join(self.data_dir, 'ghana', 's1', f\"s1_{'ghana'}_{loc_id}.json\"), 'r').read())\n",
        "        s1 = s1_json['dates']\n",
        "\n",
        "        s1 =np.array([datetime.strptime(date, \"%Y-%m-%d\") for date in s1])\n",
        "        s1_date_low = s1[s1 >= datetime.strptime('2016-06-01', \"%Y-%m-%d\")]\n",
        "        s1_date_high = s1_date_low[s1_date_low<= datetime.strptime('2016-11-30', \"%Y-%m-%d\")]\n",
        "        s1_idx_min = np.where(s1==s1_date_high[0])\n",
        "        s1_idx_max = np.where(s1==s1_date_high[-1])\n",
        "        s1_idx_min = s1_idx_min[0][0]\n",
        "        s1_idx_max = s1_idx_max[0][0]\n",
        "\n",
        "\n",
        "        s2_json = json.loads(open(os.path.join(self.data_dir, 'ghana', 's2', f\"s2_{'ghana'}_{loc_id}.json\"), 'r').read())\n",
        "        s2 = s2_json['dates']\n",
        "\n",
        "        s2 =np.array([datetime.strptime(date, \"%Y-%m-%d\") for date in s2])\n",
        "        s2_date_low = s2[s2 >= datetime.strptime('2016-07-01', \"%Y-%m-%d\")]\n",
        "        s2_date_high = s2_date_low[s2_date_low<= datetime.strptime('2016-11-30', \"%Y-%m-%d\")]\n",
        "        s2_idx_min = np.where(s2==s2_date_high[0])\n",
        "        s2_idx_max = np.where(s2==s2_date_high[-1])\n",
        "        s2_idx_min = s2_idx_min[0][0]\n",
        "        s2_idx_max = s2_idx_max[0][0]\n",
        "\n",
        "        planet_json = json.loads(open(os.path.join(self.data_dir, 'ghana', 'planet', f\"planet_{'ghana'}_{loc_id}.json\"), 'r').read())\n",
        "        planet = planet_json['dates']\n",
        "\n",
        "        planet =np.array([datetime.strptime(date, \"%Y-%m-%d\") for date in planet])\n",
        "        planet_date_low = planet[planet >= datetime.strptime('2017-07-01', \"%Y-%m-%d\")]\n",
        "        planet_date_high = planet_date_low[planet_date_low<= datetime.strptime('2017-11-30', \"%Y-%m-%d\")]\n",
        "        planet_idx_min = np.where(planet==planet_date_high[0])\n",
        "        planet_idx_max = np.where(planet==planet_date_high[-1])\n",
        "        planet_idx_min = planet_idx_min[0][0]\n",
        "        planet_idx_max = planet_idx_max[0][0]\n",
        "\n",
        "        return {\"s1_min\":s1_idx_min, \"s1_max\":s1_idx_max, \"s2_min\":s2_idx_min,\"s2_max\":s2_idx_max,\"planet_min\":planet_idx_min,\"planet_max\":planet_idx_max}\n",
        "\n",
        "    def normalization(self, grid, satellite):\n",
        "        \"\"\" Normalization based on values defined in constants.py\n",
        "        Args:\n",
        "          grid - (tensor) grid to be normalized\n",
        "          satellite - (str) describes source that grid is from (\"s1\" or \"s2\")\n",
        "        Returns:\n",
        "          grid - (tensor) a normalized version of the input grid\n",
        "        \"\"\"\n",
        "        num_bands = grid.shape[0]\n",
        "        means = MEANS[satellite]['ghana']\n",
        "        stds = STDS[satellite]['ghana']\n",
        "        grid = (grid-means[:num_bands].reshape(num_bands, 1, 1, 1))/stds[:num_bands].reshape(num_bands, 1, 1, 1)\n",
        "\n",
        "        if satellite not in ['s1', 's2', 'planet']:\n",
        "            raise ValueError(\"Incorrect normalization parameters\")\n",
        "        return grid"
      ],
      "metadata": {
        "id": "ptixpERuqUjn"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SustainBenchSubset(CropTypeMappingDataset):\n",
        "    def __init__(self, dataset, split, transform=None):\n",
        "        \"\"\"\n",
        "        This acts like torch.utils.data.Subset, but on SustainBenchDatasets.\n",
        "        We pass in transform explicitly because it can potentially vary at\n",
        "        training vs. test time, if we're using data augmentation.\n",
        "\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.dataset = dataset\n",
        "        self.transform = transform\n",
        "\n",
        "        split_mask = self.split_array == self.split_dict[split]\n",
        "        self.indices = np.where(split_mask)[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x, y = self.dataset[self.indices[idx]]\n",
        "        if self.transform is not None:\n",
        "            x = self.transform(x)\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)"
      ],
      "metadata": {
        "id": "cYQ28yM-Uh1U"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = CropTypeMappingDataset()\n",
        "\n",
        "train_dataset = SustainBenchSubset(dataset, 'train')\n",
        "val_dataset = SustainBenchSubset(dataset, 'val')\n",
        "test_dataset = SustainBenchSubset(dataset, 'test')"
      ],
      "metadata": {
        "id": "Ne5kcG3f504w"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataloader"
      ],
      "metadata": {
        "id": "gsgwJrRqAvaI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(\n",
        "                train_dataset,\n",
        "                shuffle=True, # Shuffle training dataset\n",
        "                sampler=None,\n",
        "                batch_size=10)\n",
        "\n",
        "val_loader = DataLoader(\n",
        "                val_dataset,\n",
        "                shuffle=False, # Do not shuffle eval datasets\n",
        "                sampler=None,\n",
        "                batch_size=10)\n",
        "\n",
        "test_loader = DataLoader(\n",
        "                test_dataset,\n",
        "                shuffle=False, # Do not shuffle eval datasets\n",
        "                sampler=None,\n",
        "                batch_size=10)"
      ],
      "metadata": {
        "id": "Z-LJe67O2ese"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x,y in train_loader:\n",
        "  print(x['planet'].shape)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1At28FaU4MwI",
        "outputId": "36aecb49-3996-4d3c-ce46-88a7a4216d50"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 4, 128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Swin Unet Model"
      ],
      "metadata": {
        "id": "AlRsC3QCvWXm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class cyclicShift(nn.Module):\n",
        "  def __init__(self, displacement):\n",
        "    super().__init__()\n",
        "    self.displacement = displacement\n",
        "\n",
        "  def forward(self, x):\n",
        "    return torch.roll(x, shifts=(self.displacement, self.displacement), dims = (1,2) )\n",
        "\n",
        "\n",
        "class Residual(nn.Module):\n",
        "  def __init__(self, fn):\n",
        "    super().__init__()\n",
        "    self.fn = fn\n",
        "\n",
        "  def forward(self, x, **kwargs):\n",
        "    return self.fn(x,**kwargs) + x\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "  def __init__(self, dim, fn):\n",
        "    super().__init__()\n",
        "    self.norm = nn.LayerNorm(dim)\n",
        "    self.fn = fn\n",
        "\n",
        "  def forward(self, x, **kwargs):\n",
        "    return self.norm(self.fn(x, **kwargs))\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self,dim,hidden_dim):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(dim, hidden_dim),\n",
        "        nn.GELU(),\n",
        "        nn.Linear(hidden_dim, dim)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)"
      ],
      "metadata": {
        "id": "ra0-6NNI4HfJ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mask(window_size, displacement, upper_lower,left_right):\n",
        "  mask = torch.zeros(window_size**2, window_size**2)\n",
        "\n",
        "  if upper_lower:\n",
        "    mask[-displacement*window_size:,:-displacement*window_size]=float(\"-inf\")\n",
        "    mask[:-displacement*window_size,-displacement*window_size:]=float(\"-inf\")\n",
        "\n",
        "  if left_right:\n",
        "    mask = rearrange(mask, \"(h1 w1) (h2 w2) -> h1 w1 h2 w2\", h1=window_size,h2=window_size)\n",
        "    mask[:,-displacement:, :, :-displacement] = float(\"-inf\")\n",
        "    mask[:,:-displacement, :,-displacement:] = float(\"-inf\")\n",
        "    mask = rearrange(mask, \"h1 w1 h2 w2 -> (h1 w1) (h2 w2)\")\n",
        "\n",
        "  return mask"
      ],
      "metadata": {
        "id": "nxD3n5lP4LQ5"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WindowAttension(nn.Module):\n",
        "  def __init__(self, dim, heads, head_dim, shifted, window_size, relative_pos_embedding):\n",
        "    super().__init__()\n",
        "    inner_dim = head_dim * heads\n",
        "    self.heads = heads\n",
        "    self.scale = head_dim ** -0.5\n",
        "    self.window_size = window_size\n",
        "    self.relative_pos_embedding = relative_pos_embedding\n",
        "    self.shifted = shifted\n",
        "\n",
        "    if self.shifted:\n",
        "      displacement = window_size //2\n",
        "      self.cyclic_shift = cyclicShift(-displacement)\n",
        "      self.cyclic_back_shift = cyclicShift(displacement)\n",
        "\n",
        "      self.upper_lower_mask = nn.Parameter(create_mask(window_size=window_size, displacement=displacement,\\\n",
        "                                                       upper_lower=True, left_right=False), requires_grad=False)\n",
        "\n",
        "      self.left_right_mask = nn.Parameter(create_mask(window_size=window_size, displacement=displacement,\\\n",
        "                                                       upper_lower=False, left_right=True), requires_grad=False)\n",
        "\n",
        "    self.to_qkv = nn.Linear(dim, inner_dim*3,bias=False)\n",
        "    self.pos_embedding = nn.Parameter(torch.randn(window_size**2,window_size**2))\n",
        "    self.to_out = nn.Linear(inner_dim, dim)\n",
        "\n",
        "  def forward(self,x):\n",
        "    if self.shifted:\n",
        "      x = self.cyclic_shift(x)\n",
        "\n",
        "    b, n_h, n_w, _, h = *x.shape, self.heads\n",
        "    qkv = self.to_qkv(x).chunk(3,dim=-1)\n",
        "\n",
        "    nw_h = n_h // self.window_size\n",
        "    nw_w = n_w // self.window_size\n",
        "\n",
        "    q, k, v = map(lambda t: rearrange(t, \"b (nw_h w_h) (nw_w w_w) (h d)->b h (nw_h nw_w) (w_h w_w) d\",\\\n",
        "                                      h=h,w_h=self.window_size,w_w=self.window_size), qkv)\n",
        "    # Do product similarity\n",
        "    dots = torch.einsum(\"b h w i d, b h w j d->b h w i j\", q, k) * self.scale\n",
        "    dots +=self.pos_embedding\n",
        "    if self.shifted:\n",
        "      dots[:,:,-nw_w:] +=self.upper_lower_mask\n",
        "      dots[:,:,nw_w-1::nw_w] += self.left_right_mask\n",
        "\n",
        "    attn = dots.softmax(dim=-1)\n",
        "    out = torch.einsum(\"b h w i j, b h w j d->b h w i d\", attn, v)\n",
        "\n",
        "    out = rearrange(out, \"b h (nw_h nw_w) (w_h w_w) d -> b (nw_h w_h) (nw_w w_w) (h d)\",\\\n",
        "                    h=h, w_h=self.window_size, w_w=self.window_size, nw_h=nw_h, nw_w=nw_w)\n",
        "\n",
        "    out = self.to_out(out)\n",
        "    if self.shifted:\n",
        "       out = self.cyclic_back_shift(out)\n",
        "    return out"
      ],
      "metadata": {
        "id": "MEgIDShi4LEg"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SwinBlock(nn.Module):\n",
        "  def __init__(self, dim, heads, head_dim, mlp_dim, shifted, window_size, relative_pos_embedding) :\n",
        "    super().__init__()\n",
        "    self.attention_block = Residual(PreNorm(dim, WindowAttension(dim=dim,heads=heads,head_dim=head_dim,\\\n",
        "                                                                 shifted=shifted, window_size=window_size,\\\n",
        "                                                                 relative_pos_embedding=relative_pos_embedding)))\n",
        "\n",
        "    self.mlp_block = Residual(PreNorm(dim, FeedForward(dim=dim,hidden_dim=mlp_dim)))\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.attention_block(x)\n",
        "    x = self.mlp_block(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "GxT0Gxxa4UkJ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchExpanding(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, kernel_size=3, stride=2, padding=1, output_padding=1):\n",
        "    super().__init__()\n",
        "    self.patch_expand = nn.ConvTranspose2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size,\n",
        "                                           stride=stride, padding=padding, output_padding=output_padding)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.patch_expand(x).permute(0,2,3,1)\n",
        "    return x\n",
        "\n",
        "class PatchMerging_Conv(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, downscaling_factor):\n",
        "    super().__init__()\n",
        "    self.patch_merge = nn.Conv2d(in_channels, out_channels, kernel_size=downscaling_factor,stride=downscaling_factor,padding=0)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.patch_merge(x).permute(0,2,3,1)\n",
        "    return x"
      ],
      "metadata": {
        "id": "Gf2bOGEH4UgI"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class StageModule(nn.Module):\n",
        "  def __init__(self,in_channels, hidden_dimension, layers, scaling_factor,num_heads,head_dim, window_size, relative_pos_embedding,\n",
        "               PatchMerging=True, stride=2, padding=1, output_padding=1):\n",
        "    super().__init__()\n",
        "    assert layers % 2 == 0, \"Stage layers need to be divisible by 2 for regular and shftrd block\"\n",
        "    if PatchMerging:\n",
        "      self.patch_partition = PatchMerging_Conv(in_channels=in_channels, out_channels=hidden_dimension,downscaling_factor=scaling_factor)\n",
        "    else:\n",
        "      self.patch_partition = PatchExpanding(in_channels=in_channels , out_channels=hidden_dimension, kernel_size=scaling_factor, stride=stride,\n",
        "                                            padding=padding, output_padding=output_padding)\n",
        "\n",
        "    self.layers = nn.ModuleList([])\n",
        "    for _ in range(layers//2):\n",
        "      self.layers.append(nn.ModuleList([\n",
        "          SwinBlock(dim=hidden_dimension,heads=num_heads, head_dim = head_dim, mlp_dim=hidden_dimension *4, shifted =False,\\\n",
        "                    window_size = window_size, relative_pos_embedding=relative_pos_embedding),\n",
        "\n",
        "          SwinBlock(dim=hidden_dimension,heads=num_heads, head_dim = head_dim, mlp_dim=hidden_dimension *4, shifted =True,\\\n",
        "                    window_size = window_size, relative_pos_embedding=relative_pos_embedding)\n",
        "      ]))\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.patch_partition(x)\n",
        "\n",
        "    for regular_block, shifted_block in self.layers:\n",
        "      x = regular_block (x)\n",
        "      x = shifted_block (x)\n",
        "    return x.permute(0,3,1,2)"
      ],
      "metadata": {
        "id": "8DN4HwDY4ZPM"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SwinTransformerUnet(nn.Module):\n",
        "  def __init__(self,*,hidden_dim, layers, channels=3, num_classes=5, heads=(3,6,12,24), head_dim=32,window_size=7, \\\n",
        "               downscaling_factors=(4,2,2,2), scaling_factor=(3,3,3,4), relative_pos_embedding=True):\n",
        "    super().__init__()\n",
        "    # Encoder\n",
        "    self.stage1 = StageModule(in_channels=channels, hidden_dimension=hidden_dim,layers=layers[0],\\\n",
        "                              scaling_factor=downscaling_factors[0],num_heads=heads[0],head_dim=head_dim,\\\n",
        "                              window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
        "\n",
        "    self.stage2 = StageModule(in_channels=hidden_dim, hidden_dimension=hidden_dim*2,layers=layers[1],\\\n",
        "                              scaling_factor=downscaling_factors[1],num_heads=heads[1],head_dim=head_dim,\\\n",
        "                              window_size=window_size, relative_pos_embedding=relative_pos_embedding\n",
        "                              )\n",
        "    self.stage3 = StageModule(in_channels=hidden_dim*2, hidden_dimension=hidden_dim*4,layers=layers[2],\\\n",
        "                              scaling_factor=downscaling_factors[2],num_heads=heads[2],head_dim=head_dim,\\\n",
        "                              window_size=window_size, relative_pos_embedding=relative_pos_embedding\n",
        "                              )\n",
        "    self.stage4 = StageModule(in_channels=hidden_dim*4, hidden_dimension=hidden_dim*8,layers=layers[3],\\\n",
        "                              scaling_factor=downscaling_factors[3],num_heads=heads[3],head_dim=head_dim,\\\n",
        "                              window_size=window_size, relative_pos_embedding=relative_pos_embedding\n",
        "                              )\n",
        "\n",
        "    #Decoder\n",
        "    self.stage11 = StageModule(in_channels=hidden_dim*8, hidden_dimension=hidden_dim*4,layers=layers[0],\\\n",
        "                              scaling_factor=scaling_factor[0],num_heads=heads[3],head_dim=head_dim,\\\n",
        "                              window_size=window_size, PatchMerging=False, relative_pos_embedding=relative_pos_embedding)\n",
        "\n",
        "    self.stage22 = StageModule(in_channels=hidden_dim*4+hidden_dim*4, hidden_dimension=hidden_dim*2,layers=layers[1],\\\n",
        "                              scaling_factor=scaling_factor[1],num_heads=heads[2],head_dim=head_dim,\\\n",
        "                              window_size=window_size, PatchMerging=False, relative_pos_embedding=relative_pos_embedding\n",
        "                              )\n",
        "    self.stage33 = StageModule(in_channels=hidden_dim*2+hidden_dim*2, hidden_dimension=hidden_dim,layers=layers[2],\\\n",
        "                              scaling_factor=scaling_factor[2],num_heads=heads[1],head_dim=head_dim,\\\n",
        "                              window_size=window_size, PatchMerging=False, relative_pos_embedding=relative_pos_embedding\n",
        "                              )\n",
        "    self.stage44 = StageModule(in_channels=hidden_dim+hidden_dim, hidden_dimension=hidden_dim,layers=layers[3],\\\n",
        "                              scaling_factor=scaling_factor[3],num_heads=heads[0],head_dim=head_dim,\\\n",
        "                              stride=4, padding=1, output_padding=2, window_size=window_size, PatchMerging=False,\\\n",
        "                              relative_pos_embedding=relative_pos_embedding\n",
        "                              )\n",
        "\n",
        "  def forward(self, img):\n",
        "    #Encoder\n",
        "    stage1 = self.stage1(img)\n",
        "    stage2 = self.stage2(stage1)\n",
        "    stage3 = self.stage3(stage2)\n",
        "    stage4 = self.stage4(stage3)\n",
        "\n",
        "    #Decoder\n",
        "    x = self.stage11(stage4)\n",
        "    x=torch.cat([x, stage3], dim=1)\n",
        "    x = self.stage22(x)\n",
        "    x=torch.cat([x, stage2], dim=1)\n",
        "    x = self.stage33(x)\n",
        "    x=torch.cat([x, stage1], dim=1)\n",
        "    x = self.stage44(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "sMxeJAUV4ZMd"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SwinUnet(nn.Module):\n",
        "  def __init__(self,):\n",
        "    super().__init__()\n",
        "    self.planet = SwinTransformerUnet(hidden_dim = 96, layers=(2,6,2,2),heads=(3,6,12,24),channels=4,num_classes=5,head_dim=32,\\\n",
        "             window_size=2,relative_pos_embedding=True)\n",
        "\n",
        "    self.final_planet = nn.Conv2d(in_channels=96, out_channels=96, stride= 2, kernel_size=2 ,padding= 0)\n",
        "\n",
        "    self.s1 = SwinTransformerUnet(hidden_dim = 96, layers=(2,6,2,2),heads=(3,6,12,24),channels=3,num_classes=5,head_dim=32,\\\n",
        "             window_size=2,relative_pos_embedding=True)\n",
        "\n",
        "    self.s2 = SwinTransformerUnet(hidden_dim = 96, layers=(2,6,2,2),heads=(3,6,12,24),channels=10,num_classes=5,head_dim=32,\\\n",
        "             window_size=2,relative_pos_embedding=True)\n",
        "\n",
        "    self.final=nn.Conv2d(in_channels=288, out_channels=5, kernel_size=3,\n",
        "                               stride=1, padding=1)\n",
        "    self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "  def forward(self,X):\n",
        "\n",
        "\n",
        "    planet = self.final_planet(self.planet(X['planet'].to(DEVICE)))\n",
        "    s1 = self.s1(X['s1'].to(DEVICE))\n",
        "    s2 = self.s2(X['s2'].to(DEVICE))\n",
        "    out = self.final(torch.cat([s1, s2, planet], dim=1))\n",
        "    return self.softmax(out)"
      ],
      "metadata": {
        "id": "2_L4UnI_4ZEO"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model object\n",
        "model_swin = SwinUnet().to(DEVICE)\n"
      ],
      "metadata": {
        "id": "YflCiStTqz1p"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_weights = 1 - np.array([.85, .17, .56, .16, .11])\n",
        "loss_weights = torch.tensor(loss_weights, dtype=torch.float32).cuda()\n",
        "\n",
        "config = {\n",
        "    \"epochs\"           : 100,\n",
        "    \"lr\"               : 0.001,\n",
        "    \"label_smoothing\"  : 0.2,\n",
        "    \"momentum\"         : 0.9,\n",
        "    \"weight_decay\"     : 0.0001,\n",
        "    \"loss_weights\"     : loss_weights\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "id": "QQMrjeTmrDjJ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMFZ5Q3UesvA"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wki28zN9esvA",
        "outputId": "cd58ddc1-15da-42c0-be21-4dffeeb58462"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbwirayesu\u001b[0m (\u001b[33mwn\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "wandb.login(key=\"< >\") #API Key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "2USolO4MesvB",
        "outputId": "9a661bb4-0441-476a-9eba-e8474c730669"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "wandb version 0.16.1 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20231206_105936-u4ujzh2v</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Resuming run <strong><a href='https://wandb.ai/wn/Crop-type-segmentation/runs/u4ujzh2v' target=\"_blank\">SwinT-Ghana-indivTime-1</a></strong> to <a href='https://wandb.ai/wn/Crop-type-segmentation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/wn/Crop-type-segmentation' target=\"_blank\">https://wandb.ai/wn/Crop-type-segmentation</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/wn/Crop-type-segmentation/runs/u4ujzh2v' target=\"_blank\">https://wandb.ai/wn/Crop-type-segmentation/runs/u4ujzh2v</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "run = wandb.init(\n",
        "    name = \"SwinT-Ghana-indivTime-1\", ## Wandb creates random run names if you skip this field\n",
        "    # reinit = True, ### Allows reinitalizing runs when you re-run this cell\n",
        "    id ='u4ujzh2v', ### Insert specific run id here if you want to resume a previous run\n",
        "    resume = \"must\", ### You need this to resume previous runs, but comment out reinit = True when using this\n",
        "    project = \"Crop-type-segmentation\", ### Project should be created in your wandb account\n",
        "     ### Wandb Config for your run\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSyt3BWE9RzM",
        "outputId": "53097afc-4277-4a5d-9391-385c53d278e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# metrics, train and validation functions\n",
        "def reshapeForLoss(y):\n",
        "    \"\"\" Reshapes labels or preds for loss fn.\n",
        "    To get them to the correct shape, we permute:\n",
        "      [batch x classes x rows x cols] --> [batch x rows x cols x classes]\n",
        "      and then reshape to [N x classes], where N = batch*rows*cols\n",
        "    \"\"\"\n",
        "    # [batch x classes x rows x cols] --> [batch x rows x cols x classes]\n",
        "    y = y.permute(0, 2, 3, 1)\n",
        "    # [batch x rows x cols x classes] --> [batch*rows*cols x classes]\n",
        "    y = y.contiguous().view(-1, y.shape[3])\n",
        "    return y"
      ],
      "metadata": {
        "id": "WdzAvndyp9RJ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FugQI96hjnmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mask_ce_loss(y_true, y_pred, weight_scale=1):\n",
        "\n",
        "    y_true = reshapeForLoss(y_true)\n",
        "    num_examples = torch.sum(y_true, dtype=torch.float32).cuda()\n",
        "    y_pred = reshapeForLoss(y_pred)\n",
        "\n",
        "    loss_mask = torch.sum(y_true, dim=1).type(torch.LongTensor)\n",
        "    loss_mask_repeat = loss_mask.unsqueeze(1).repeat(1,y_pred.shape[1]).type(torch.FloatTensor)\n",
        "    _, y_true = torch.max(y_true, dim=1)\n",
        "    y_true = y_true * loss_mask\n",
        "    y_pred_ = y_pred * loss_mask_repeat.cuda()\n",
        "\n",
        "\n",
        "    loss_fn = nn.NLLLoss(weight = config[\"loss_weights\"] ** weight_scale)\n",
        "\n",
        "    total_loss = torch.sum(loss_fn(y_pred, y_true.cuda()))\n",
        "\n",
        "    if num_examples == 0:\n",
        "        print(\"WARNING: NUMBER OF EXAMPLES IS 0\")\n",
        "\n",
        "    else: return total_loss / num_examples"
      ],
      "metadata": {
        "id": "km57aBesQCrG"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "optimizer = torch.optim.SGD(model_swin.parameters(), lr=config[\"lr\"], momentum=config[\"momentum\"], weight_decay=config[\"weight_decay\"])\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100, eta_min=0.00005)\n"
      ],
      "metadata": {
        "id": "upjuupIuUVjW"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Metrics\n",
        "def crop_segmentation_metrics(y_true, y_pred):\n",
        "        y_true = reshapeForLoss(y_true.cpu())\n",
        "        y_pred = reshapeForLoss(y_pred.cpu())\n",
        "\n",
        "        loss_mask = torch.sum(y_true, dim=1).type(torch.LongTensor)\n",
        "\n",
        "        _, y_true = torch.max(y_true, dim=1)\n",
        "        _, y_pred = torch.max(y_pred, dim=1)\n",
        "\n",
        "        y_true = y_true[loss_mask == 1]\n",
        "        y_pred = y_pred[loss_mask == 1]\n",
        "\n",
        "        assert (y_true.shape == y_pred.shape)\n",
        "        y_true = y_true.int()\n",
        "        y_pred = y_pred.int()\n",
        "\n",
        "        acc = accuracy_score(y_true, y_pred)\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "        return acc, cm\n",
        "\n",
        "# Train\n",
        "def train_step(data_loader, optimizer, accuracy_fn):\n",
        "  \"\"\"Performs a training with model trying to learn on data-loader\"\"\"\n",
        "\n",
        "  curr_lr = float(optimizer.param_groups[0]['lr'])\n",
        "  model_swin.train() # Put model into training mode\n",
        "\n",
        "  train_loss, train_acc = 0,0\n",
        "  # Add a loop to loop through the training batches\n",
        "  for img,label in tqdm(data_loader):\n",
        "\n",
        "    # 1. Forwad pass\n",
        "    y_pred=model_swin(img)\n",
        "\n",
        "    # 2. Calculate loss and accuracy (per batch)\n",
        "    label = label.long()\n",
        "    label = torch.nn.functional.one_hot(label, num_classes=5).permute([0,3,1,2])\n",
        "    # label = label.to(DEVICE)\n",
        "\n",
        "\n",
        "    loss=mask_ce_loss(label, y_pred)\n",
        "\n",
        "    train_loss+=loss.item() #  accumulate training loss\n",
        "    train_acc += accuracy_fn(y_true=label, y_pred=y_pred)[0]\n",
        "\n",
        "\n",
        "    # 3. optimize zero grad\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # 4. Loss backward\n",
        "    loss.backward()\n",
        "\n",
        "    # 5. Optimizer step\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "  # Divide total train loss and acc by lenth of train dataloader\n",
        "  train_loss/=len(data_loader)\n",
        "  train_acc/=len(data_loader)\n",
        "  print(f\"Train loss {train_loss: .5f}|Train acc : {train_acc:.4f} | lr: {curr_lr}\\n\")\n",
        "  return train_loss, train_acc, curr_lr\n",
        "\n",
        "# Validation\n",
        "def validation_step(data_loader, accuracy_fn):\n",
        "  \"\"\"Performs a tesing loop step on model going over data loader.\"\"\"\n",
        "\n",
        "  val_loss,val_acc = 0,0\n",
        "\n",
        "  model_swin.eval() # put the model in eval mode\n",
        "  # turn on inference mode context manager\n",
        "  with torch.inference_mode():\n",
        "    for img,label in tqdm(data_loader):\n",
        "\n",
        "        # 1. Forward pass (outputs raw logits)\n",
        "      val_pred=model_swin(img)\n",
        "\n",
        "      # 2. Calculate the loss/acc\n",
        "      label = label.long()\n",
        "      label = torch.nn.functional.one_hot(label, num_classes=5).permute([0,3,1,2])\n",
        "      # label = label.to(DEVICE)\n",
        "      val_loss+=mask_ce_loss(label, val_pred)\n",
        "\n",
        "      # val_pred_masked = torch.max(val_pred,dim=1)[1]\n",
        "      val_acc += accuracy_fn(y_true=label, y_pred=val_pred)[0]\n",
        "\n",
        "    # Adjust metrics and print out\n",
        "    val_loss/=len(data_loader)\n",
        "    val_acc/=len(data_loader)\n",
        "    print(f\"Val loss: {val_loss:.5f} | Val acc: {val_acc:.4f} \\n\")\n",
        "\n",
        "    return val_loss, val_acc"
      ],
      "metadata": {
        "id": "rMU5b1M6sq5x"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "ynDh0nClAGxw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's train\n",
        "epochs=config['epochs']\n",
        "\n",
        "# Create an optimization and evluation using train_step() and val_step()\n",
        "train_loss_list=[]\n",
        "train_acc_list=[]\n",
        "\n",
        "val_loss_list=[]\n",
        "val_acc_list=[]\n",
        "\n",
        "\n",
        "best_val_acc = float('inf')\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "  print(f\"Epoch: {epoch}/{epochs} \\n----------------\")\n",
        "\n",
        "\n",
        "  train_loss, train_acc, curr_lr = train_step(\n",
        "                                                data_loader=train_loader,\n",
        "                                                optimizer=optimizer,\n",
        "                                                accuracy_fn=crop_segmentation_metrics\n",
        "                                                )\n",
        "  train_loss_list.append(train_loss)\n",
        "  train_acc_list.append(train_acc)\n",
        "\n",
        "  val_loss, val_acc = validation_step(\n",
        "                                            data_loader=val_loader,\n",
        "                                            accuracy_fn=crop_segmentation_metrics,\n",
        "                                            )\n",
        "  val_loss_list.append(val_loss)\n",
        "  val_acc_list.append(val_acc)\n",
        "\n",
        "  wandb.log({\"train_loss\": train_loss, 'train_acc': train_acc,\n",
        "               'validation_loss': val_loss, 'validation_acc': val_acc, \"learning_Rate\": curr_lr})\n",
        "\n",
        "  if val_acc < val_acc:\n",
        "      best_val_acc = val_acc\n",
        "      torch.save(model_swin.state_dict(), './models/best.pth')  # Save the best model\n",
        "\n",
        "      print(\"Saving model\")\n",
        "      torch.save({'model_state_dict':model_swin.state_dict(),\n",
        "                'optimizer_state_dict':optimizer.state_dict(),\n",
        "                'scheduler_state_dict':scheduler.state_dict(),\n",
        "                'best_val_acc': best_val_acc,\n",
        "                'epoch': epoch}, './models/best.pth')\n",
        "\n",
        "      wandb.save('./models/best.pth')\n",
        "\n",
        "  scheduler.step()\n",
        "\n",
        "run.finish()\n"
      ],
      "metadata": {
        "id": "ue9jxHh94Ism"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "model_swin.load_state_dict(torch.load('/content/models/best_.pth')['model_state_dict'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EijOJ907CY8",
        "outputId": "a5bd0983-7f4e-4292-923f-fdfb419a86fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_swin.eval() # put the model in eval mode\n",
        "# turn on inference mode context manager\n",
        "test_acc = 0\n",
        "with torch.inference_mode():\n",
        "  for img,label in tqdm(test_loader):\n",
        "\n",
        "      # 1. Forward pass (outputs raw logits)\n",
        "    val_pred=model_swin(img)\n",
        "\n",
        "    # 2. Calculate the loss/acc\n",
        "    label = label.long()\n",
        "    label = torch.nn.functional.one_hot(label, num_classes=5).permute([0,3,1,2])\n",
        "\n",
        "    test_acc += crop_segmentation_metrics(y_true=label, y_pred=val_pred)[0]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UYI19Ec94Ux9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjust metrics and print out\n",
        "test_acc/=len(test_loader)\n",
        "print(f\"test acc: {test_acc:.4f}\")"
      ],
      "metadata": {
        "id": "dlR1_BgO9ho4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JXi4FYXi9fS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6u_S_j_q1Cx2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uCQbmBuZjnNq"
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}